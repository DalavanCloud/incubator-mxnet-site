{"nbformat": 4, "cells": [{"source": "# Hybrid - Faster training and easy deployment\n\n*Related Content:*\n* [Fast, portable neural networks with Gluon HybridBlocks](https://gluon.mxnet.io/chapter07_distributed-learning/hybridize.html)\n* [A Hybrid of Imperative and Symbolic Programming\n](http://en.diveintodeeplearning.org/chapter_computational-performance/hybridize.html)\n\nDeep learning frameworks can be roughly divided into two categories: declarative\nand imperative. With declarative frameworks (including Tensorflow, Theano, etc)\nusers first declare a fixed computation graph and then execute it end-to-end.\nThe benefit of fixed computation graph is it's portable and runs more\nefficiently. However, it's less flexible because any logic must be encoded\ninto the graph as special operators like `scan`, `while_loop` and `cond`.\nIt's also hard to debug.\n\nImperative frameworks (including PyTorch, Chainer, etc) are just the opposite:\nthey execute commands one-by-one just like old fashioned Matlab and Numpy.\nThis style is more flexible, easier to debug, but less efficient.\n\n`HybridBlock` seamlessly combines declarative programming and imperative programming\nto offer the benefit of both. Users can quickly develop and debug models with\nimperative programming and switch to efficient declarative execution by simply\ncalling: `HybridBlock.hybridize()`.\n\n## HybridBlock\n\n`HybridBlock` is very similar to `Block` but has a few restrictions:\n\n- All children layers of `HybridBlock` must also be `HybridBlock`.\n- Only methods that are implemented for both `NDArray` and `Symbol` can be used.\n  For example you cannot use `.asnumpy()`, `.shape`, etc.\n- Operations cannot change from run to run. For example, you cannot do `if x:`\n  if `x` is different for each iteration.\n\nTo use hybrid support, we subclass the `HybridBlock`:", "cell_type": "markdown", "metadata": {}}, {"source": "import mxnet as mx\nfrom mxnet import gluon\nfrom mxnet.gluon import nn\n\nmx.random.seed(42)\n\nclass Net(gluon.HybridBlock):\n    def __init__(self, **kwargs):\n        super(Net, self).__init__(**kwargs)\n        with self.name_scope():\n            # layers created in name_scope will inherit name space\n            # from parent layer.\n            self.conv1 = nn.Conv2D(6, kernel_size=5)\n            self.pool1 = nn.MaxPool2D(pool_size=2)\n            self.conv2 = nn.Conv2D(16, kernel_size=5)\n            self.pool2 = nn.MaxPool2D(pool_size=2)\n            self.fc1 = nn.Dense(120)\n            self.fc2 = nn.Dense(84)\n            # You can use a Dense layer for fc3 but we do dot product manually\n            # here for illustration purposes.\n            self.fc3_weight = self.params.get('fc3_weight', shape=(10, 84))\n\n    def hybrid_forward(self, F, x, fc3_weight):\n        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n        # and fc3_weight is either self.fc3_weight.data() or\n        # self.fc3_weight.var() depending on whether x is Symbol or NDArray\n        print(x)\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        # 0 means copy over size from corresponding dimension.\n        # -1 means infer size from the rest of dimensions.\n        x = x.reshape((0, -1))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.dot(x, fc3_weight, transpose_b=True)\n        return x", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Hybridize\n\nBy default, `HybridBlock` runs just like a standard `Block`. Each time a layer\nis called, its `hybrid_forward` will be run:", "cell_type": "markdown", "metadata": {}}, {"source": "net = Net()\nnet.initialize()\nx = mx.nd.random_normal(shape=(16, 1, 28, 28))\nnet(x)\nx = mx.nd.random_normal(shape=(16, 1, 28, 28))\nnet(x)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Hybrid execution can be activated by simply calling `.hybridize()` on the top\nlevel layer. The first forward call after activation will try to build a\ncomputation graph from `hybrid_forward` and cache it. On subsequent forward\ncalls the cached graph, instead of `hybrid_forward`, will be invoked:", "cell_type": "markdown", "metadata": {}}, {"source": "net.hybridize()\nx = mx.nd.random_normal(shape=(16, 1, 28, 28))\nnet(x)\nx = mx.nd.random_normal(shape=(16, 1, 28, 28))\nnet(x)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Note that before hybridize, `print(x)` printed out one NDArray for forward,\nbut after hybridize, only the first forward printed out a Symbol. On subsequent\nforward `hybrid_forward` is not called so nothing was printed.\n\nHybridize will speed up execution and save memory. If the top level layer is\nnot a `HybridBlock`, you can still call `.hybridize()` on it and Gluon will try\nto hybridize its children layers instead.\n\n`hybridize` also accepts several options for performance tuning. For example, you\ncan do", "cell_type": "markdown", "metadata": {}}, {"source": "net.hybridize(static_alloc=True)\n# or\nnet.hybridize(static_alloc=True, static_shape=True)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Please refer to the [API manual](https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=hybridize#mxnet.gluon.Block.hybridize)\nfor details.\n\n## Serializing trained model for deployment\n\nModels implemented as `HybridBlock` can be easily serialized. The serialized\nmodel can be loaded back later or used for deployment\nwith other language front-ends like C, C++ and Scala. To this end, we simply\nuse `export` and `SymbolBlock.imports`:", "cell_type": "markdown", "metadata": {}}, {"source": "net(x)\nnet.export('model', epoch=1)", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "Two files `model-symbol.json` and `model-0001.params` are saved on disk.\nYou can use other language bindings to load them. You can also load them back\nto gluon with `SymbolBlock`:", "cell_type": "markdown", "metadata": {}}, {"source": "net2 = gluon.SymbolBlock.imports('model-symbol.json', ['data'], 'model-0001.params')", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "## Operators that do not work with hybridize\n\nIf you want to hybridize your model, you must use `F.some_operator` in your 'hybrid_forward' function.\n`F` will be `mxnet.nd` before you hybridize and `mxnet.sym` after hybridize. While most APIs are the same in NDArray and Symbol, there are some differences. Writing `F.some_operator` and call `hybridize` may not work all of the time.\nHere we list some frequently used NDArray APIs that can't be hybridized and provide you the work arounds.  \n\n### Element-wise Operators\n\nIn NDArray APIs, the following arithmetic and comparison APIs are automatically broadcasted if the input NDArrays have different shapes.\nHowever, that's not the case in Symbol API. It's not automatically broadcasted, and you have to manually specify to use another set of broadcast operators for Symbols expected to have different shapes.\n\n\n| NDArray APIs  | Description  |\n|---|---|\n| [*NDArray.\\__add\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__add__) | x.\\__add\\__(y) <=> x+y <=> mx.nd.add(x, y)  |\n| [*NDArray.\\__sub\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__sub__) | x.\\__sub\\__(y) <=> x-y <=> mx.nd.subtract(x, y)  |\n| [*NDArray.\\__mul\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__mul__) | x.\\__mul\\__(y) <=> x*y <=> mx.nd.multiply(x, y)  |\n| [*NDArray.\\__div\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__div__) | x.\\__div\\__(y) <=> x/y <=> mx.nd.divide(x, y)  |\n| [*NDArray.\\__mod\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__mod__) | x.\\__mod\\__(y) <=> x%y <=> mx.nd.modulo(x, y)  |\n| [*NDArray.\\__lt\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__lt__) |  x.\\__lt\\__(y) <=> x<y <=> x mx.nd.lesser(x, y) |\n| [*NDArray.\\__le\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__le__) |  x.\\__le\\__(y) <=> x<=y <=> mx.nd.less_equal(x, y) |\n| [*NDArray.\\__gt\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__gt__) |  x.\\__gt\\__(y) <=> x>y <=> mx.nd.greater(x, y) |\n| [*NDArray.\\__ge\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__ge__) |  x.\\__ge\\__(y) <=> x>=y <=> mx.nd.greater_equal(x, y)|\n| [*NDArray.\\__eq\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__eq__) |  x.\\__eq\\__(y) <=> x==y <=> mx.nd.equal(x, y) |\n| [*NDArray.\\__ne\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__ne__) |  x.\\__ne\\__(y) <=> x!=y <=> mx.nd.not_equal(x, y) |\n\nThe current workaround is to use corresponding broadcast operators for arithmetic and comparison to avoid potential hybridization failure when input shapes are different.\n\n| Symbol APIs  | Description  |\n|---|---|\n|[*broadcast_add*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_add) | Returns element-wise sum of the input arrays with broadcasting. |\n|[*broadcast_sub*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_sub) | Returns element-wise difference of the input arrays with broadcasting. |\n|[*broadcast_mul*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_mul) | Returns element-wise product of the input arrays with broadcasting. |\n|[*broadcast_div*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_div) | Returns element-wise division of the input arrays with broadcasting. |\n|[*broadcast_mod*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_mod) | Returns element-wise modulo of the input arrays with broadcasting. |\n|[*broadcast_equal*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_equal) | Returns the result of element-wise *equal to* (==) comparison operation with broadcasting. |\n|[*broadcast_not_equal*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_not_equal) | Returns the result of element-wise *not equal to* (!=) comparison operation with broadcasting. |\n|[*broadcast_greater*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_greater) | Returns the result of element-wise *greater than* (>) comparison operation with broadcasting. |\n|[*broadcast_greater_equal*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_greater_equal) | Returns the result of element-wise *greater than or equal to* (>=) comparison operation with broadcasting. |\n|[*broadcast_lesser*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_lesser) |\tReturns the result of element-wise *lesser than* (<) comparison operation with broadcasting. |\n|[*broadcast_lesser_equal*](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.broadcast_lesser_equal) | Returns the result of element-wise *lesser than or equal to* (<=) comparison operation with broadcasting. |\n\nFor example, if you want to add a NDarray to your input x, use `broadcast_add` instead of `+`:", "cell_type": "markdown", "metadata": {}}, {"source": "def hybrid_forward(self, F, x):\n    # avoid writing: return x + F.ones((1, 1))\n    return F.broadcast_add(x, F.ones((1, 1)))", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "If you used `+`, it would still work before hybridization, but will throw an error of shape missmtach after hybridization.\n\n### Shape\n\nGluon's imperative interface is very flexible and allows you to print the shape of the NDArray. However, Symbol does not have shape attributes. As a result, you need to avoid printing shapes in `hybrid_forward`.\nOtherwise, you will get the following error:", "cell_type": "markdown", "metadata": {}}, {"source": "AttributeError: 'Symbol' object has no attribute 'shape'", "cell_type": "markdown", "metadata": {}}, {"source": "### Slice\n`[]` in NDArray is used to get a slice from the array. However, `[]` in Symbol is used to get an output from a grouped symbol.\nFor example, you will get different results for the following method before and after hybridization.", "cell_type": "markdown", "metadata": {}}, {"source": "def hybrid_forward(self, F, x):\n    return x[0]", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {}}, {"source": "\nThe current workaround is to explicitly call [`slice`](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.slice) or [`slice_axis`](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.slice_axis) operators in `hybrid_forward`.\n\n\n### Not implemented operators\n\nSome of the often used operators in NDArray are not implemented in Symbol, and will cause hybridization failure.\n\n#### NDArray.asnumpy\nSymbol does not support the `asnumpy` function. You need to avoid calling `asnumpy` in `hybrid_forward`.\n\n#### Array creation APIs\n\n`mx.nd.array()` is used a lot, but Symbol does not have the `array` API. The current workaround is to use `F.ones`, `F.zeros`, or `F.full`, which exist in both the NDArray and Symbol APIs.\n\n#### In-Place Arithmetic Operators\n\nIn-place arithmetic operators may be used in Gluon imperative mode, however if you expect to hybridize, you should write these operations explicitly instead.\nFor example, avoid writing `x += y` and use `x  = x + y`, otherwise you will get `NotImplementedError`. This applies to all the following operators:\n\n| NDArray in-place arithmetic operators | Description |\n|---|---|\n|[*NDArray.\\__iadd\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__iadd__) |\tx.\\__iadd\\__(y) <=> x+=y |\n|[*NDArray.\\__isub\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__isub__) |\tx.\\__isub\\__(y) <=> x-=y |\n|[*NDArray.\\__imul\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__imul__) |\tx.\\__imul\\__(y) <=> x*=y |\n|[*NDArray.\\__idiv\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__idiv__) |\tx.\\__rdiv\\__(y) <=> x/=y |\n|[*NDArray.\\__imod\\__*](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.__imod__) |\tx.\\__rmod\\__(y) <=> x%=y |\n\n\n\n## Summary\n\nThe recommended practice is to utilize the flexibility of imperative NDArray API during experimentation. Once you finalized your model, make necessary changes mentioned above so you can call `hybridize` function to improve performance.\n\n<!-- INSERT SOURCE DOWNLOAD BUTTONS -->", "cell_type": "markdown", "metadata": {}}], "metadata": {"display_name": "", "name": "", "language": "python"}, "nbformat_minor": 2}